{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                             INVESCO TRADING - RISK FOR REDEEMING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<b> OVERVIEW: </b>\n",
    "\n",
    "Invesco is an independent investment management firm which sells retail mutual funds across The United States of America via financial intermediaries. They would like to understand who is at risk for selling(Redeeming) their mutual fund in the next month and which mutual funds are at a risk so that they can take a proactive approach targeted specifically at the risk areas.\n",
    "\n",
    "They have a wealth of internal data regarding the purchase and sale of their mutual funds , asset holding balances , product investment experience information and financial intermediary activity information. The same had been provided to us for analysis.\n",
    "\n",
    "<b> Team: </b> \n",
    "\n",
    "Vallabh Remani - Intern,\n",
    "Koti - Software Engineer,\n",
    "Seshu - Director of Engineering, \n",
    "Sri Harsha - Mentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>WORKFLOW STRUCTURE</b>\n",
    "\n",
    "To solve the given problem, we have taken a 7 step approach, each step dealing with one particular section of the solution development. The steps are as follows:\n",
    "\n",
    "    \n",
    "    1. Question or problem definition.\n",
    "    2. Acquire training and testing data.\n",
    "    3. Wrangle, prepare, cleanse the data.\n",
    "    4. Analyze, identify patterns, and explore the data.\n",
    "    5. Model, predict and solve the problem.\n",
    "    6. Visualize, report, and present the problem solving steps and final solution.\n",
    "    7. Supply or submit the results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. QUESTION OR PROBLEM DEFINITION</b>\n",
    "\n",
    "Having a training set of samples listing whether the investment has been redeemed by the advisor or not, can our model predict whether a particular investment_id will be redeemed or not on the test dataset whose redeemption status is unknown. If yes, What will be the accuracy of the system.\n",
    "\n",
    "To solve the above problem, we had to develop some early understanding of the problem domain by consulting experts in that particular field as this would give us a clear understanding of the factors that would influence the outcome.\n",
    "\n",
    "The highlights of the understanding were as follows:\n",
    "<ul>\n",
    "<li> Advisors are more likely to buy funds that have been around for atleast 3 years as this would ensure security</li>\n",
    "<li> There types of advisors are varied. Some look at short term ratings and short term gains and others look at long term ratings and long term gains </li>\n",
    "<li> The more times the advisor got involved in any activity the more are his chances of purchasing the fund in the following month.</li>\n",
    "<li> Advisors generally care more about buying funds rather than selling them off meaning, the advisor would sell off a particular fund when they have a better fund to invest the same money in.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. ACQUIRING TRAINING AND TESTING DATASET</b>\n",
    "\n",
    "The dataset was made available as 4 csv files.\n",
    "<ul>\n",
    "<li>InvestmentExperience.csv</li>\n",
    "<li>Transaction.csv</li>\n",
    "<li>AUM.csv</li>\n",
    "<li>Activity.csv</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing all the necessary packages and Libraries which will help us in solving the above mentioned problem.\n",
    "\n",
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.1 ACQUIRING DATA</b>\n",
    "\n",
    "The python pandas and numpy packages help us work with our dataset. We load the csv files into the pandas and numpy dataframes for further data processing and data analysis.\n",
    "\n",
    "Let us load the datasets into the pandas dataframes and have an initial understanding of the structure of the data provided to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the Investment Experience Dataset into the pandas dataframe.\n",
    "invexp_df = pd.read_csv('resources\\dataset\\InvestmentExperience.csv')\n",
    "\n",
    "# Loading the Transaction Dataset into the pandas dataframe.\n",
    "transaction_df = pd.read_csv('resources\\dataset\\Transaction.csv')\n",
    "\n",
    "# Loading the AUM Dataset into the pandas dataframe.\n",
    "aum_df = pd.read_csv('resources\\dataset\\AUM.csv')\n",
    "\n",
    "# Loading the Activity Dataset into the pandas dataframe.\n",
    "activity_df = pd.read_csv('resources\\dataset\\Activity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.2 ANALYZE BY DESCRIBING THE DATA</b>\n",
    "\n",
    "Let us try to understand the features that are available in the data provided to us.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Knowing the features in the Investment Experience Table\n",
    "print(invexp_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<b>Which features are categorical?</b>\n",
    "\n",
    "These values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\n",
    "\n",
    "   <ul><li>Categorical: Morningstar Category , Investment. </li>\n",
    "   <li>Ordinal: Rating.</li></ul>\n",
    "\n",
    "<b>Which features are numerical?</b>\n",
    "\n",
    "Which features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\n",
    " \n",
    "   <ul><li>Discrete: Unique_Investment_Id</li>\n",
    "   <li>Timeseries: Month</li>\n",
    "   <li>Continous: All others</li>\n",
    "   </ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preview the Data\n",
    "\n",
    "invexp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Knowing the features in the Transaction Table\n",
    "print(transaction_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Which features are categorical?</b>\n",
    "\n",
    "These values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\n",
    "\n",
    "   <ul><li>Categorical: Transaction_Type </li>\n",
    "   </ul>\n",
    "\n",
    "<b>Which features are numerical?</b>\n",
    "\n",
    "Which features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\n",
    " \n",
    "   <ul><li>Discrete: Unique_Advisor_Id , Unique_Investment_Id , Code_1 , Code_2 , Code_3 , Code_4 , Code_5  </li>\n",
    "   <li>Timeseries: Month</li>\n",
    "   <li>Continous: Amount</li>\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preview the Data\n",
    "\n",
    "transaction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Knowing the features in the Transaction Table\n",
    "print(aum_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Which features are categorical?</b>\n",
    "\n",
    "These values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\n",
    "\n",
    "   <ul><li>Categorical: None </li>\n",
    "   </ul>\n",
    "\n",
    "<b>Which features are numerical?</b>\n",
    "\n",
    "Which features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\n",
    " \n",
    "   <ul><li>Discrete: Unique_Advisor_Id , Unique_Investment_Id   </li>\n",
    "   <li>Timeseries: Month</li>\n",
    "   <li>Continous: Shares , AUM</li>\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preview the Data\n",
    "\n",
    "aum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Knowing the features in the Transaction Table\n",
    "print(activity_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Which features are categorical?</b>\n",
    "\n",
    "These values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\n",
    "\n",
    "   <ul><li>Categorical: Activity_Type </li>\n",
    "   </ul>\n",
    "\n",
    "<b>Which features are numerical?</b>\n",
    "\n",
    "Which features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\n",
    " \n",
    "   <ul><li>Discrete: Unique_Advisor_Id , Activity_Count   </li>\n",
    "   <li>Timeseries: Month</li>\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preview the Data\n",
    "\n",
    "activity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Knowing the number of Unique_Investment_Id's that come under each Morningstar Category\n",
    "\n",
    "invexp_df[['Morningstar Category', 'Unique_Investment_Id']].groupby(['Morningstar Category'], as_index=False).count().sort_values(by='Morningstar Category', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Knowing the number of Unique_Investment_Id's that come under each Morningstar Category\n",
    "\n",
    "invexp_df[['Month', 'Unique_Investment_Id']].groupby(['Month'], as_index=False).count().sort_values(by='Month', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that every month, Investments have been made on 592 Investments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Knowing the number of Transactions of Type 'Redeem' in each month\n",
    "transaction_df = transaction_df[transaction_df.Transaction_Type == 'R']\n",
    "transaction_df[['Month', 'Transaction_Type']].groupby(['Month'], as_index=False).count().sort_values(by='Month', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Knowing the number of Transactions of Type 'Purchase' in each month\n",
    "transaction_df = pd.read_csv('resources\\dataset\\Transaction.csv')\n",
    "transaction_df = transaction_df[transaction_df.Transaction_Type == 'P']\n",
    "transaction_df[['Month', 'Transaction_Type']].groupby(['Month'], as_index=False).count().sort_values(by='Month', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of Investments done by an advisor in a given month \n",
    "\n",
    "aum_df[['Unique_Advisor_Id', 'Unique_Investment_Id' , 'Month']].groupby(['Unique_Advisor_Id' , 'Month'], as_index=False).count().sort_values(by='Month', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of Activities done by Each Advisor in each month\n",
    "\n",
    "activity_df[['Unique_Advisor_Id', 'Month', 'Activity_Count']].groupby(['Unique_Advisor_Id' , 'Month'], as_index=False).sum().sort_values(by='Month', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.3 VISUALIZING THE GIVEN DATA FOR ANALYTICS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transaction amount in various months for a particular Advisor = 1000103\n",
    "\n",
    "transaction_df = pd.read_csv('resources\\dataset\\Transaction.csv')\n",
    "transaction_df = transaction_df[transaction_df.Unique_Advisor_Id == 1000103]\n",
    "transaction_df.plot('Month', 'Amount', kind='bar', color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the AUM for the advisor = 12243 and Investment = 11681.\n",
    "\n",
    "aum_df = aum_df[aum_df.Unique_Advisor_Id == 12243]\n",
    "aum_df = aum_df[aum_df.Unique_Investment_Id == 11681 ]\n",
    "aum_df.plot('Month' , 'AUM' , kind='bar' , color = 'b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For a given Advisor = 1000103 , Find the type of activities that he does.\n",
    "\n",
    "activity_df = pd.read_csv('resources\\dataset\\Activity.csv')\n",
    "activity_df = activity_df[activity_df.Unique_Advisor_Id == 1000103]\n",
    "activity_df.plot('Activity_Type' , 'Activity_Count' , kind='bar' , color = 'g')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. WRANGLE , PREPARE AND CLEAN THE DATA</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Correcting by dropping features</b>\n",
    "\n",
    "This is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our processing and eases the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will try to see which columns are unnecessary for our analysis and drop them.\n",
    "\n",
    "invexp_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that the Morningstar Category and Investment Columns are not necessary for our computation, so we will delete those columns from our table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "invexp_df = pd.read_csv('resources\\dataset\\InvestmentExperience.csv')\n",
    "invexp_df = invexp_df.drop(['Morningstar Category', 'Investment'], axis=1)\n",
    "invexp_df.columns.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will try to see which columns are unnecessary for our analysis and drop them.\n",
    "\n",
    "transaction_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that Code_1 , Code_2 .. Code_5 are not required for the computation, so we are dropping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transaction_df = transaction_df.drop(['Code_1', 'Code_2', 'Code_3', 'Code_4', 'Code_5',], axis=1)\n",
    "transaction_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will try to see which columns are unnecessary for our analysis and drop them.\n",
    "\n",
    "aum_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that all the features are important in this table, so not dropping any of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will try to see which columns are unnecessary for our analysis and drop them.\n",
    "\n",
    "activity_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume all the attributes are important for the final computation, so leaving everything as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to join all the dataframes so that we can create one dataframe with all the necessary details. We need to use the join statements in order to join them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merging the tables according to common attributes\n",
    "data = pd.merge(aum_df, activity_df, how='inner', on=['Unique_Advisor_Id', 'Month']).fillna(0.0)\n",
    "data = pd.merge(data, invexp_df, how='inner', on=['Unique_Investment_Id', 'Month']).fillna(0.0)\n",
    "\n",
    "# Writing this data to a csv file called feature_vector.csv for future use.\n",
    "data.to_csv(myconfig.PROCESSED_DATASET_FOLDER + myconfig.SEP + \"feature_vector.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looking at all the values in the table . Check if they have been combined or not.\n",
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the class_label to X and changing it later on to the desired value and attaching the class label to each row \n",
    "# Here, The class label of each class depends on the feature values of the previous months.\n",
    "# Using the values of months and using a for loop, we are joining the value of 'P' or 'R' to each combination of Unique_Investment_Id and Unique_Advisor_Id\n",
    "# Writing the final table into a csv file called dataset.csv for future use.\n",
    "\n",
    "data['class_label'] = 'X'\n",
    "\n",
    "print(\"Length of dataset: \" + str(len(data)))\n",
    "for rowidx in range(len(data)):\n",
    "    class_label = None\n",
    "    this_month = data.get_value(rowidx, 'Month')\n",
    "    next_month = invesco.get_next_month(this_month)\n",
    "\n",
    "    this_aid = data.get_value(rowidx, 'Unique_Advisor_Id')\n",
    "    this_iid = data.get_value(rowidx, 'Unique_Investment_Id')\n",
    "    t1 = tnx[tnx.Unique_Advisor_Id == this_aid]\n",
    "    t2 = t1[t1.Unique_Investment_Id == this_iid]\n",
    "    t3 = t2[t2.Month == next_month]\n",
    "\n",
    "    if len(t3) == 0:\n",
    "        class_label = 'H'\n",
    "    else:\n",
    "        p_amount = 0.0\n",
    "        r_amount = 0.0\n",
    "        for rowidx in range(len(t3)):\n",
    "            txn_type = t3.iloc[rowidx, t3.columns.get_loc('Transaction_Type')]\n",
    "            amt_str = t3.iloc[rowidx, t3.columns.get_loc('Amount')]\n",
    "            print(\"AID : %s / IID: %s / Row Idx: %s  / Amount: %s\" % (this_aid, this_iid, rowidx, amt_str))\n",
    "            amount = abs(float(amt_str))\n",
    "            if txn_type == 'P':\n",
    "                p_amount = amount\n",
    "            else:\n",
    "                r_amount = amount\n",
    "        if r_amount > p_amount:\n",
    "            class_label = 'R'\n",
    "        else:\n",
    "            class_label = 'P'\n",
    "    data.iloc[rowidx, data.columns.get_loc('class_label')] = class_label\n",
    "\n",
    "data.to_csv(myconfig.PROCESSED_DATASET_FOLDER + myconfig.SEP + \"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4. MODEL, PREDICT AND SOLVE</b>\n",
    "\n",
    "Now we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n",
    "\n",
    "   <ul><li> Logistic Regression</li>\n",
    "    <li>KNN or k-Nearest Neighbors</li>\n",
    "    <li>Support Vector Machines</li>\n",
    "    <li>Naive Bayes classifier</li>\n",
    "    <li>Decision Tree</li>\n",
    "    <li>Random Forrest</li>\n",
    "    <li>Perceptron</li>\n",
    "    <li>Artificial neural network</li>\n",
    "    <li>RVM or Relevance Vector Machine</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the available dataset, we will divide the available dataset into training dataset and test dataset in the ratio 80:20 as is the standard convention.\n",
    "\n",
    "We will then run the available machine learning algorithms on the dataset to see which one produces the highest output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6. VISUALIZE, REPORT AND SOLVE THE PROBLEM</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6.1 VERSION 1 SOLUTION:</b>\n",
    "\n",
    "Looking at the data provided to us, we might have to predict the status for investments and advisors for whom complete information has not been provided.\n",
    "\n",
    "The method to solve this problem would involve creating a similarity matrix using the Collaborative Filtering Algorithm. This similarity matrix can be constructed both based on users(advisors) or items(investments). The basic idea is that we will try to find the users who are similar to the given user and we will try to predict that the given user will take decisions like the user he is most similar to.\n",
    "\n",
    "The same logic applies when we try to build the item item similarity matrix. We try to find items that are similar to each other and for predicting which item will be redeemed, we will try to see the status of the item whose information is known to us.\n",
    "\n",
    "This solution is inspired by the BellKor Solution to the netflix problem which had to find similarity between users who watch movies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply Collaborative filtering \n",
    "    - Try both item - item based and user - user based approach.\n",
    "\"\"\"\n",
    "from src.main.core import invesco\n",
    "from src.main.core import myconfig\n",
    "import pandas as pd\n",
    "\n",
    "__EPSILON = 1e-9\n",
    "\n",
    "\n",
    "def compute_cfmat(month=None):\n",
    "    df = invesco.get_txn_df()\n",
    "    if month:\n",
    "        df = df[df.Month <= invesco.get_datetime(month)]\n",
    "\n",
    "    print(\"#Rows: \" + str(len(df)))\n",
    "    x = pd.DataFrame(df, columns=[df.Unique_Advisor_Id.name, df.Unique_Investment_Id.name, df.Transaction_Type.name])\n",
    "    x['count'] = 0\n",
    "\n",
    "    y = x.groupby([x.Unique_Advisor_Id.name, x.Unique_Investment_Id.name, x.Transaction_Type]).count().reset_index()\n",
    "\n",
    "    pcounts = y[y.Transaction_Type == 'P']\n",
    "    rcounts = y[y.Transaction_Type == 'R']\n",
    "\n",
    "    pmat = pd.DataFrame(pcounts, columns=[pcounts.Unique_Advisor_Id.name, pcounts.Unique_Investment_Id.name, 'count'])\n",
    "    rmat = pd.DataFrame(rcounts, columns=[rcounts.Unique_Advisor_Id.name, rcounts.Unique_Investment_Id.name, 'count'])\n",
    "\n",
    "    users = set()\n",
    "    items = set()\n",
    "\n",
    "    users = users.union(set(pmat[pmat.Unique_Advisor_Id.name].unique()))\n",
    "    users = users.union(set(rmat[rmat.Unique_Advisor_Id.name].unique()))\n",
    "\n",
    "    items = items.union(set(pmat[pmat.Unique_Investment_Id.name].unique()))\n",
    "    items = items.union(set(rmat[rmat.Unique_Investment_Id.name].unique()))\n",
    "\n",
    "    print(\"USERS: \\n\")\n",
    "    print(users)\n",
    "\n",
    "    print(\"\\nITEMS\")\n",
    "    print(items)\n",
    "\n",
    "    columns = list(sorted(list(items)))\n",
    "    rows = list(sorted(list(users)))\n",
    "\n",
    "    cfmat = pd.DataFrame(index=rows, columns=columns).fillna(0)\n",
    "\n",
    "    for user in users:\n",
    "        for item in items:\n",
    "            pvalc = 0.0\n",
    "            rvalc = 0.0\n",
    "\n",
    "            rval = rmat[rmat.Unique_Advisor_Id == user]\n",
    "            rval = rval[rval.Unique_Investment_Id == item]\n",
    "            if (len(rval) > 0):\n",
    "                rvalc = float(rval['count'])\n",
    "\n",
    "            pval = pmat[pmat.Unique_Advisor_Id == user]\n",
    "            pval = pval[pval.Unique_Investment_Id == item]\n",
    "            if (len(pval) > 0):\n",
    "                pvalc = float(pval['count'])\n",
    "\n",
    "            prob = float(rvalc / (__EPSILON + rvalc + pvalc))\n",
    "            # print(\"user : \" + user + \" / item: \" + item + \" / pvalc : \" + str(pvalc) + \" / rvalc : \" + str(rvalc) + \" / prob: \" + str(prob))\n",
    "            cfmat.loc[user, item] = prob\n",
    "    return cfmat\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cfmat = compute_cfmat(month='2016 / 10')\n",
    "    # pmat.to_csv(myconfig.PROCESSED_DATASET_FOLDER + myconfig.SEP + \"purchase_mat.csv\")\n",
    "    # rmat.to_csv(myconfig.PROCESSED_DATASET_FOLDER + myconfig.SEP + \"redeem_mat.csv\")\n",
    "    cfmat.to_csv(myconfig.PROCESSED_DATASET_FOLDER + myconfig.SEP + \"cfmat.csv\")\n",
    "    print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above piece of code, we have tried to calculate the number of times the transaction of 'P' and the number of times of the transaction 'R' have happend. This detail is stored for every user and every item whose data is available.\n",
    "\n",
    "This is then used to predict values for unknown data using the similarity approach already talked about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "from src.main.core.algorithms import txnmat\n",
    "\n",
    "\n",
    "class CF1(object):\n",
    "    def __init__(self, month=None):\n",
    "        self.iid2idx = dict()\n",
    "        self.iid2idx = dict()\n",
    "        self.idx2iid = dict()\n",
    "        self.aid2idx = dict()\n",
    "        self.idx2aid = dict()\n",
    "        self.n_users = -1\n",
    "        self.n_items = -1\n",
    "        self.month = month\n",
    "        self.__load()\n",
    "\n",
    "    def __get_mse(self, pred, actual):\n",
    "        # Ignore nonzero terms.\n",
    "        pred = pred[actual.nonzero()].flatten()\n",
    "        actual = actual[actual.nonzero()].flatten()\n",
    "        return mean_squared_error(pred, actual)\n",
    "\n",
    "    def __measure_sparsity(self, mat):\n",
    "        sparsity = float(len(mat.nonzero()[0]))\n",
    "        sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "        sparsity *= 100\n",
    "        return 'Sparsity: {:4.2f}%'.format(sparsity)\n",
    "\n",
    "    def __train_test_split(self, datamat, size=2):\n",
    "        test = np.zeros(datamat.shape)\n",
    "        train = datamat.copy()\n",
    "        for user in range(datamat.shape[0]):\n",
    "            non_zeros = datamat[user, :].nonzero()[0]\n",
    "            if (len(non_zeros) > size):\n",
    "                test_ratings = np.random.choice(datamat[user, :].nonzero()[0], size=size, replace=False)\n",
    "                train[user, test_ratings] = 0.\n",
    "                test[user, test_ratings] = datamat[user, test_ratings]\n",
    "\n",
    "        # Test and training are truly disjoint\n",
    "        assert (np.all((train * test) == 0))\n",
    "        return train, test\n",
    "\n",
    "    def __fast_similarity(self, datamat, kind='user', epsilon=1e-9):\n",
    "        # epsilon -> small number for handling dived-by-zero errors\n",
    "        if kind == 'user':\n",
    "            sim = datamat.dot(datamat.T) + epsilon\n",
    "        elif kind == 'item':\n",
    "            sim = datamat.T.dot(datamat) + epsilon\n",
    "        norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "        return (sim / norms / norms.T)\n",
    "\n",
    "    def __predict_slow_simple(self, datamat, similarity, kind='user'):\n",
    "        pred = np.zeros(datamat.shape)\n",
    "        if kind == 'user':\n",
    "            for i in range(datamat.shape[0]):\n",
    "                for j in range(datamat.shape[1]):\n",
    "                    pred[i, j] = similarity[i, :].dot(datamat[:, j]) \\\n",
    "                                 / np.sum(np.abs(similarity[i, :]))\n",
    "            return pred\n",
    "        elif kind == 'item':\n",
    "            for i in range(datamat.shape[0]):\n",
    "                for j in range(datamat.shape[1]):\n",
    "                    pred[i, j] = similarity[j, :].dot(datamat[i, :].T) \\\n",
    "                                 / np.sum(np.abs(similarity[j, :]))\n",
    "\n",
    "            return pred\n",
    "\n",
    "    def __predict_fast_simple(self, ratings, similarity, kind='user'):\n",
    "        if kind == 'user':\n",
    "            return similarity.dot(ratings) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "        elif kind == 'item':\n",
    "            return ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "\n",
    "    def __print_data(self, matrix, message):\n",
    "        print(\"\\n\")\n",
    "        print(\"Dataset: \" + message)\n",
    "        print(\"Shape: \" + str(matrix.shape))\n",
    "        print(\"Sparsity: \" + str(self.__measure_sparsity(matrix)))\n",
    "        print(\"First 5 rows: \")\n",
    "        print(matrix[:5, ])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def __load_index_map(self, df):\n",
    "        self.iids = list(df.columns[1:])\n",
    "        for idx, iid in enumerate(self.iids):\n",
    "            self.iid2idx[iid] = idx\n",
    "            self.idx2iid[idx] = iid\n",
    "\n",
    "        self.aids = list(df.index)\n",
    "        for idx, aid in enumerate(self.aids):\n",
    "            self.aid2idx[aid] = idx\n",
    "            self.idx2aid[idx] = aid\n",
    "\n",
    "        self.n_users = len(self.aids)\n",
    "        self.n_items = len(self.iids)\n",
    "\n",
    "    def __load(self):\n",
    "        df = txnmat.compute_cfmat(month=self.month)\n",
    "        self.__load_index_map(df)\n",
    "        ratings = df.as_matrix(df.columns[1:])\n",
    "\n",
    "        # print_data(ratings, \"Ratings matrix\")\n",
    "        train, test = self.__train_test_split(ratings)\n",
    "        # print_data(train, \"Training Matrix\")\n",
    "        # print_data(test, \"Testing Matrix\")\n",
    "\n",
    "        user_similarity = self.__fast_similarity(ratings)\n",
    "        item_similarity = self.__fast_similarity(ratings, kind='item')\n",
    "\n",
    "        # print_data(user_similarity, \"User Similarity Matrix\")\n",
    "        # print_data(item_similarity, \"Item Similarity Matrix\")\n",
    "\n",
    "        self.item_prediction = self.__predict_fast_simple(train, item_similarity, kind='item')\n",
    "        self.user_prediction = self.__predict_fast_simple(train, user_similarity, kind='user')\n",
    "\n",
    "        print('User-based CF MSE: ' + str(self.__get_mse(self.user_prediction, test)))\n",
    "        print('Item-based CF MSE: ' + str(self.__get_mse(self.item_prediction, test)))\n",
    "        # print_data(item_prediction, \"Item Pred\")\n",
    "        # print_data(user_prediction, \"User Pred\")\n",
    "\n",
    "    def get_value(self, aid, iid, algorithm='user'):\n",
    "        if aid in self.aid2idx:\n",
    "            aid_idx = self.aid2idx[aid]\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "        if iid in self.iid2idx:\n",
    "            iid_idx = self.iid2idx[iid]\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "        value = -1\n",
    "        if algorithm == 'user':\n",
    "            value = self.user_prediction[aid_idx][iid_idx]\n",
    "        elif algorithm == 'item':\n",
    "            value = self.item_prediction[aid_idx][iid_idx]\n",
    "        else:\n",
    "            print(\"Invalid algorithm input provided.\")\n",
    "        return value\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method was then used to predict the future action based on the history of the item or user. This method gave us an accuracy of 28.15% on the testing dataset. This has been established as the ground truth. \n",
    "\n",
    "We will now build solutions that will be able to develop on the solution already provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6.2 BETTERING THE VERSION 1 SOLUTION</b>\n",
    "\n",
    "Let us now use the Machine Learning Algorithms Like Logistic Regression to find the solution to the above problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries and packages for applying machine learning algorithms.\n",
    "# Logistic Regression\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv('resources\\processed_dataset\\dataset.csv')\n",
    "df.replace('NaN' , -9999 , inplace=True)\n",
    "df.drop(['Unique_Advisor_Id' , 'Unique_Investment_Id' , 'Month' , 'Morningstar Category' , 'Investment'] , 1 , inplace = True)\n",
    "\n",
    "x = np.array(df.drop(['class_label'] , 1))\n",
    "y = np.array(df['class_label'])\n",
    "\n",
    "x_train , x_test , y_train , y_test = model_selection.train_test_split(x , y , test_size = 0.2)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train , y_train)\n",
    "\n",
    "def predict(self,df):\n",
    "    self.df = df\n",
    "    output = logreg.predict(self.df)\n",
    "    print(output)\n",
    "\n",
    "print(logreg.predict_proba(x_test))\n",
    "\n",
    "accuracy_of_logistic = logreg.score(x_train, y_train)\n",
    "print(accuracy_of_logistic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We got an accuracy of 96.0006437596% using the Logistic Regression Algorithm on the dataset that we have.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We have used the Logistic Regression model because we wanted to know both the class to which a particular input belongs and also the probability that a particular input belongs to a class. </b>\n",
    "\n",
    "Had we no restriction on finding out the probabilty of the input belonging to any class, we could have used the other Machine Learning Algorithms whose codes are available below.\n",
    "\n",
    "In this particular problem, we have applied logistic regression and stopped but in other problems, it may be the case that we have to run all the possible algorithms on the data and see the algorithm that gives the highest accuracy.\n",
    "\n",
    "Then double down on the algorithm that give you the highest accuracy and use that algorithm to make furthur predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using SVM\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(x_train, y_train)\n",
    "Y_pred = svc.predict(x_test)\n",
    "acc_svc = round(svc.score(x_train, y_train) * 100, 2)\n",
    "acc_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using kNN \n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict(x_test)\n",
    "acc_knn = round(knn.score(x_train, y_train) * 100, 2)\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train, Y_train)\n",
    "Y_pred = gaussian.predict(X_test)\n",
    "acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n",
    "acc_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, Y_train)\n",
    "Y_pred = perceptron.predict(X_test)\n",
    "acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n",
    "acc_perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear SVC\n",
    "\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, Y_train)\n",
    "Y_pred = linear_svc.predict(X_test)\n",
    "acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n",
    "acc_linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, Y_train)\n",
    "Y_pred = sgd.predict(X_test)\n",
    "acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n",
    "acc_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, Y_train)\n",
    "Y_pred = decision_tree.predict(X_test)\n",
    "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "random_forest.score(X_train, Y_train)\n",
    "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
    "acc_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>7. SUPPLY OR SUBMIT THE RESULTS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the code ready and working well on the test dataset with an accuracy of 96.0006437596% , Given any dataset on which we have to predict the output, We can take the dataset and run the above mentioned algorithm to get the predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
